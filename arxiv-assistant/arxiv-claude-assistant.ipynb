{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7V1M4ktoTLaF"
      },
      "outputs": [],
      "source": [
        "import arxiv\n",
        "import json\n",
        "import os\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "import anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv python-dotenv anthropic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75GMv_pcTNm5",
        "outputId": "9550b52a-96cc-4527-c3b9-1cac542b6099"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting anthropic\n",
            "  Downloading anthropic-0.52.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading anthropic-0.52.0-py3-none-any.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=10bf8b527800384a92fea4f534389a71687ef5768477a9720f284f67e5f69c94\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, python-dotenv, feedparser, arxiv, anthropic\n",
            "Successfully installed anthropic-0.52.0 arxiv-2.2.0 feedparser-6.0.11 python-dotenv-1.1.0 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAPER_DIR = \"papers\""
      ],
      "metadata": {
        "id": "oJFOGWjrTT-J"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first tool searches for relevant arXiv papers based on a topic and stores the papers' info in a JSON file (title, authors, summary, paper url and the publication date). The JSON files are organized by topics in the papers directory. The tool does not download the papers."
      ],
      "metadata": {
        "id": "fQcUuF9ZTdkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_papers(topic: str, max_results: int = 5) -> List[str]:\n",
        "    \"\"\"\n",
        "    Search for papers on arXiv based on a topic and store their information.\n",
        "\n",
        "    Args:\n",
        "        topic: The topic to search for\n",
        "        max_results: Maximum number of results to retrieve (default: 5)\n",
        "\n",
        "    Returns:\n",
        "        List of paper IDs found in the search\n",
        "    \"\"\"\n",
        "\n",
        "    # Use arxiv to find the papers\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    # Search for the most relevant articles matching the queried topic\n",
        "    search = arxiv.Search(\n",
        "        query = topic,\n",
        "        max_results = max_results,\n",
        "        sort_by = arxiv.SortCriterion.Relevance\n",
        "    )\n",
        "\n",
        "    papers = client.results(search)\n",
        "\n",
        "    # Create directory for this topic\n",
        "    path = os.path.join(PAPER_DIR, topic.lower().replace(\" \", \"_\"))\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    file_path = os.path.join(path, \"papers_info.json\")\n",
        "\n",
        "    # Try to load existing papers info\n",
        "    try:\n",
        "        with open(file_path, \"r\") as json_file:\n",
        "            papers_info = json.load(json_file)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        papers_info = {}\n",
        "\n",
        "    # Process each paper and add to papers_info\n",
        "    paper_ids = []\n",
        "    for paper in papers:\n",
        "        paper_ids.append(paper.get_short_id())\n",
        "        paper_info = {\n",
        "            'title': paper.title,\n",
        "            'authors': [author.name for author in paper.authors],\n",
        "            'summary': paper.summary,\n",
        "            'pdf_url': paper.pdf_url,\n",
        "            'published': str(paper.published.date())\n",
        "        }\n",
        "        papers_info[paper.get_short_id()] = paper_info\n",
        "\n",
        "    # Save updated papers_info to json file\n",
        "    with open(file_path, \"w\") as json_file:\n",
        "        json.dump(papers_info, json_file, indent=2)\n",
        "\n",
        "    print(f\"Results are saved in: {file_path}\")\n",
        "\n",
        "    return paper_ids"
      ],
      "metadata": {
        "id": "hreYrWnzTcp8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_papers(\"computers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRMZLoxZTqxx",
        "outputId": "f352520a-429e-477b-c354-afca25b28056"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results are saved in: papers/computers/papers_info.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1310.7911v2',\n",
              " 'math/9711204v1',\n",
              " '2208.00733v1',\n",
              " '2504.07020v1',\n",
              " '2403.03925v1']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second tool looks for information about a specific paper across all topic directories inside the papers directory."
      ],
      "metadata": {
        "id": "OgxhhEyDUefm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_info(paper_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Search for information about a specific paper across all topic directories.\n",
        "\n",
        "    Args:\n",
        "        paper_id: The ID of the paper to look for\n",
        "\n",
        "    Returns:\n",
        "        JSON string with paper information if found, error message if not found\n",
        "    \"\"\"\n",
        "\n",
        "    for item in os.listdir(PAPER_DIR):\n",
        "        item_path = os.path.join(PAPER_DIR, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            file_path = os.path.join(item_path, \"papers_info.json\")\n",
        "            if os.path.isfile(file_path):\n",
        "                try:\n",
        "                    with open(file_path, \"r\") as json_file:\n",
        "                        papers_info = json.load(json_file)\n",
        "                        if paper_id in papers_info:\n",
        "                            return json.dumps(papers_info[paper_id], indent=2)\n",
        "                except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "                    print(f\"Error reading {file_path}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "    return f\"There's no saved information related to paper {paper_id}.\""
      ],
      "metadata": {
        "id": "wGMvoOtvTtPT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_info('1310.7911v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "Is1DU3ANUl86",
        "outputId": "f90d5b5c-39ae-4052-9da2-288fecee6ba0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n  \"title\": \"Compact manifolds with computable boundaries\",\\n  \"authors\": [\\n    \"Zvonko Iljazovic\"\\n  ],\\n  \"summary\": \"We investigate conditions under which a co-computably enumerable closed set\\\\nin a computable metric space is computable and prove that in each locally\\\\ncomputable computable metric space each co-computably enumerable compact\\\\nmanifold with computable boundary is computable. In fact, we examine the notion\\\\nof a semi-computable compact set and we prove a more general result: in any\\\\ncomputable metric space each semi-computable compact manifold with computable\\\\nboundary is computable. In particular, each semi-computable compact\\\\n(boundaryless) manifold is computable.\",\\n  \"pdf_url\": \"http://arxiv.org/pdf/1310.7911v2\",\\n  \"published\": \"2013-10-29\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool Schema\n",
        "\n",
        "Here are the schema of each tool which you will provide to the LLM."
      ],
      "metadata": {
        "id": "3XWM3QwUVASD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"name\": \"search_papers\",\n",
        "        \"description\": \"Search for papers on arXiv based on a topic and store their information.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"topic\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The topic to search for\"\n",
        "                },\n",
        "                \"max_results\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"Maximum number of results to retrieve\",\n",
        "                    \"default\": 5\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"topic\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"extract_info\",\n",
        "        \"description\": \"Search for information about a specific paper across all topic directories.\",\n",
        "        \"input_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"paper_id\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The ID of the paper to look for\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"paper_id\"]\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "G2p30jp4UrDY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool mapping\n",
        "\n",
        "This code handles tool mapping and execution."
      ],
      "metadata": {
        "id": "7sbzZWn1VdzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_tool_function = {\n",
        "    \"search_papers\": search_papers,\n",
        "    \"extract_info\": extract_info\n",
        "}\n",
        "\n",
        "def execute_tool(tool_name, tool_args):\n",
        "\n",
        "    result = mapping_tool_function[tool_name](**tool_args)\n",
        "\n",
        "    if result is None:\n",
        "        result = \"The operation completed but didn't return any results.\"\n",
        "\n",
        "    elif isinstance(result, list):\n",
        "        result = ', '.join(result)\n",
        "\n",
        "    elif isinstance(result, dict):\n",
        "        # Convert dictionaries to formatted JSON strings\n",
        "        result = json.dumps(result, indent=2)\n",
        "\n",
        "    else:\n",
        "        # For any other type, convert using str()\n",
        "        result = str(result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "aEDq3MuyU-9a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = anthropic.Anthropic(api_key='')"
      ],
      "metadata": {
        "id": "a-WJ3BqeVdCJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(query):\n",
        "\n",
        "    messages = [{'role': 'user', 'content': query}]\n",
        "\n",
        "    response = client.messages.create(max_tokens = 2024,\n",
        "                                  model = 'claude-3-7-sonnet-20250219',\n",
        "                                  tools = tools,\n",
        "                                  messages = messages)\n",
        "\n",
        "    process_query = True\n",
        "    while process_query:\n",
        "        assistant_content = []\n",
        "\n",
        "        for content in response.content:\n",
        "            if content.type == 'text':\n",
        "\n",
        "                print(content.text)\n",
        "                assistant_content.append(content)\n",
        "\n",
        "                if len(response.content) == 1:\n",
        "                    process_query = False\n",
        "\n",
        "            elif content.type == 'tool_use':\n",
        "\n",
        "                assistant_content.append(content)\n",
        "                messages.append({'role': 'assistant', 'content': assistant_content})\n",
        "\n",
        "                tool_id = content.id\n",
        "                tool_args = content.input\n",
        "                tool_name = content.name\n",
        "                print(f\"Calling tool {tool_name} with args {tool_args}\")\n",
        "\n",
        "                result = execute_tool(tool_name, tool_args)\n",
        "                messages.append({\"role\": \"user\",\n",
        "                                  \"content\": [\n",
        "                                      {\n",
        "                                          \"type\": \"tool_result\",\n",
        "                                          \"tool_use_id\": tool_id,\n",
        "                                          \"content\": result\n",
        "                                      }\n",
        "                                  ]\n",
        "                                })\n",
        "                response = client.messages.create(max_tokens = 2024,\n",
        "                                  model = 'claude-3-7-sonnet-20250219',\n",
        "                                  tools = tools,\n",
        "                                  messages = messages)\n",
        "\n",
        "                if len(response.content) == 1 and response.content[0].type == \"text\":\n",
        "                    print(response.content[0].text)\n",
        "                    process_query = False"
      ],
      "metadata": {
        "id": "_Il3OG4CYKFg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_loop():\n",
        "    print(\"Type your queries or 'quit' to exit.\")\n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"\\nQuery: \").strip()\n",
        "            if query.lower() == 'quit':\n",
        "                break\n",
        "\n",
        "            process_query(query)\n",
        "            print(\"\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError: {str(e)}\")"
      ],
      "metadata": {
        "id": "taM6A_UxZWuE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e76-fP8zZdK-",
        "outputId": "12fe1a06-9c0d-457b-e914-4c8131792eae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type your queries or 'quit' to exit.\n",
            "\n",
            "Query: Search for papers on \"LLM interpretability\"\n",
            "I'll search for papers on LLM interpretability for you. Let me retrieve that information.\n",
            "Calling tool search_papers with args {'topic': 'LLM interpretability', 'max_results': 5}\n",
            "Results are saved in: papers/llm_interpretability/papers_info.json\n",
            "I've found 5 papers related to LLM interpretability. Here are the paper IDs:\n",
            "\n",
            "1. 2412.07992v3\n",
            "2. 2402.01761v1 \n",
            "3. 2407.04307v1\n",
            "4. 2408.13006v2\n",
            "5. 2308.08728v1\n",
            "\n",
            "Would you like me to extract more detailed information about any of these specific papers? If so, please let me know which paper ID you're interested in.\n",
            "\n",
            "\n",
            "\n",
            "Query: Give me a summary of this paper 2412.07992v3\n",
            "I'll help you find information about this paper. The ID you provided (2412.07992v3) appears to be an arXiv paper ID. Let me use the extract_info function to retrieve details about this specific paper.\n",
            "Calling tool extract_info with args {'paper_id': '2412.07992v3'}\n",
            "# Summary of \"Concept Bottleneck Large Language Models\"\n",
            "\n",
            "## Paper Information\n",
            "- **Title**: Concept Bottleneck Large Language Models (CB-LLMs)\n",
            "- **Authors**: Chung-En Sun, Tuomas Oikarinen, Berk Ustun, Tsui-Wei Weng\n",
            "- **Published**: December 11, 2024\n",
            "- **arXiv ID**: 2412.07992v3\n",
            "\n",
            "## Key Points\n",
            "\n",
            "This paper introduces Concept Bottleneck Large Language Models (CB-LLMs), a framework that builds interpretability directly into Large Language Models rather than relying on post-hoc interpretation methods. The key innovation is that CB-LLMs integrate intrinsic interpretability into the model architecture itself.\n",
            "\n",
            "The researchers developed CB-LLMs for two main NLP tasks:\n",
            "\n",
            "1. **Text Classification**: The CB-LLM approach achieves competitive and sometimes superior performance compared to traditional black-box models while providing explicit and interpretable reasoning for its decisions.\n",
            "\n",
            "2. **Text Generation**: The model incorporates interpretable neurons that enable:\n",
            "   - Precise concept detection\n",
            "   - Controlled text generation\n",
            "   - Safer outputs\n",
            "\n",
            "## Benefits and Applications\n",
            "\n",
            "The built-in interpretability of CB-LLMs offers several advantages:\n",
            "- Allows users to transparently identify harmful content\n",
            "- Enables steering of model behavior\n",
            "- Provides mechanisms to unlearn undesired concepts\n",
            "\n",
            "These capabilities significantly enhance the safety, reliability, and trustworthiness of LLMs - features that are notably lacking in existing models.\n",
            "\n",
            "The paper's code is publicly available at: https://github.com/Trustworthy-ML-Lab/CB-LLMs\n",
            "\n",
            "\n",
            "\n",
            "Query: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGU-dtP3ZhkV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}